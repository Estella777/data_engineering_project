{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "180408e2-d727-44bf-95c0-79e319e2e789",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a table in Postgres first, need to connect to Postgresql first\n",
    "# insert data in chuncks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "da63c4f0-67d4-44ec-abdf-3b17edc41a7b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sqlalchemy import create_engine\n",
    "import pyarrow.parquet as pq\n",
    "from sqlalchemy import text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "feb5fdc9-c199-4eb8-ab44-8943b31a4a59",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'2.2.2'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.__version__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "7ef7efb4-0de9-4473-a48b-51d1eba38ce6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# use this way to just get df file and get DDL command to create table. To actually load data in chunks, use pyarrow down below\n",
    "df = pd.read_parquet('/Users/estelle/Downloads/yellow_tripdata_2021-01.parquet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "ab244de4-1f0f-4fd3-be56-f3fa7fbf76cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "table_name = 'yellow_taxi_data'\n",
    "chunk_size = 100000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "3966dd0a-03ed-4995-8535-8519d55c312d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# to change data type, this can be omitted here\n",
    "#df.to_datetime(df.tpep_pickup_datetime)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "8db616a1-f10e-4efe-8ef8-f1c40ce1350b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#user:password@host:portal/database_name\n",
    "engine = create_engine('postgresql://root:root@localhost:5432/ny_taxi')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "d8ed50f6-0851-4e4c-9f92-df657a890614",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<sqlalchemy.engine.base.Connection at 0x10da49110>"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#test if engine works\n",
    "#engine.connect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "baad810e-eb14-4000-932d-cadfd1740a65",
   "metadata": {},
   "outputs": [],
   "source": [
    "#only can be used in Pandas, this uses df to generate DDL command used to create a table in Postgres\n",
    "create_table_sql = pd.io.sql.get_schema(df, name='yellow_taxi_data', con=engine)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "747879e7-c483-4d4a-816c-77dae88749a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "#create table in Postgresql\n",
    "with engine.connect() as conn:\n",
    "    conn.execute(text(f\"DROP TABLE IF EXISTS {table_name};\"))  # Optional: drop if exists\n",
    "    conn.execute(text(create_table_sql)) # text: This wraps your raw SQL string in a sqlalchemy.sql.text object, which allows SQLAlchemy to safely execute raw SQL.\n",
    "    conn.commit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "b3c4b773-e3f9-4e70-9404-6c5c80d9b089",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total rows in file:1,369,769\n"
     ]
    }
   ],
   "source": [
    "#load data in chuncks\n",
    "pf = pq.ParquetFile('/Users/estelle/Downloads/yellow_tripdata_2021-01.parquet')\n",
    "total_rows = pf.metadata.num_rows\n",
    "print(f'Total rows in file:{total_rows:,}') # add , to do thousands"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "84cec4b5-73ba-4bee-9e91-bce4a27fddc7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Inserted rows 1 to 100,000\n",
      "Inserted rows 100001 to 200,000\n",
      "Inserted rows 200001 to 300,000\n",
      "Inserted rows 300001 to 400,000\n",
      "Inserted rows 400001 to 500,000\n",
      "Inserted rows 500001 to 600,000\n",
      "Inserted rows 600001 to 700,000\n",
      "Inserted rows 700001 to 800,000\n",
      "Inserted rows 800001 to 900,000\n",
      "Inserted rows 900001 to 1,000,000\n",
      "Inserted rows 1000001 to 1,100,000\n",
      "Inserted rows 1100001 to 1,200,000\n",
      "Inserted rows 1200001 to 1,300,000\n",
      "Inserted rows 1300001 to 1,369,769\n",
      "Done loading all chunks.\n"
     ]
    }
   ],
   "source": [
    "#using pyarrow to load in chunck\n",
    "# first Read all data (all row groups) into a PyArrow Table, then slice it\n",
    "for i in range(0, total_rows, chunk_size):\n",
    "    #pf.num_row_groups return number of groups, and range create a list like in a for loop\n",
    "    table_chunck = pf.read_row_groups(range(pf.num_row_groups)).slice(i, chunk_size)\n",
    "    df_chunk = table_chunck.to_pandas()\n",
    "    df_chunk.to_sql(table_name, engine, if_exists='append', index = False)\n",
    "    print(f'Inserted rows {i+1} to {min(i + chunk_size, total_rows):,}')\n",
    "print('Done loading all chunks.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "457b0c4c-b9e3-48ea-a3d0-d763553f33b5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "dc9a9307-99b2-4615-8945-b0425c741e06",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "769"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34449b32-6bd2-4e35-a06c-254d3ab42808",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
